# Full-VTN - ViT and Full Self Attention Transformer

# General input/ouput info
frames: 256 # 32
num_classes: 17 # Clip level, multi-label classification 
img_size: 224
spatial_frozen: False


# Spatial Transformer
# spatial_args:
#   img_size: 224
#   in_chans: 3
#   attn_drop_rate: 0.0
#   drop_rate: 0.0

# Temporal Transformer
temporal_type: transformer
temporal_args:
  dim: 128
  depth: 6 # best at 9
  heads: 8 # num of multi-head attention in 1 encoder block, best at 12
  dim_head: 128
  mlp_dim: 4096 #4096
  dropout: 0.1